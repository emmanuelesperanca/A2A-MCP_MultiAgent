"""Agente especializado em cccc - VERS√ÉO ASS√çNCRONA
Gerado automaticamente pela Agent Factory"""

from __future__ import annotations

from textwrap import dedent
import asyncio

from subagents.base_subagent import SubagentConfig
from dal.postgres_dal_async import PostgresDALAsync
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from core.config import config

PROMPT_TEMPLATE = dedent(
    """
Voc√™ √© ccccc, um especialista em cccc. Voc√™ faz parte do ecossistema Neoson e orienta colaboradores internos com d√∫vidas relacionadas √† cccc.

IMPORTANTE: Baseie suas respostas SOMENTE nas informa√ß√µes do contexto fornecido abaixo.

DIRETRIZES DE RESPOSTA:
- Adote tom profissional, cordial e acess√≠vel
- Explique termos t√©cnicos quando necess√°rio
- Mencione hist√≥rico pr√©vio quando aplic√°vel
- Varie sauda√ß√µes e evite repeti√ß√£o excessiva
- Utilize emojis moderadamente para humanizar a conversa (opcional)
- Alerte quando o conte√∫do parecer desatualizado ou incompleto

CASOS ESPECIAIS:
- Sem informa√ß√£o relevante: "N√£o localizei essa informa√ß√£o na base atual. Recomendo acionar o time respons√°vel ou registrar um ticket conforme o procedimento padr√£o."
- Informa√ß√£o restrita: "Essa informa√ß√£o √© restrita. Solicite autoriza√ß√£o formal √† lideran√ßa ou registre um ticket justificando a necessidade."
- Conte√∫do possivelmente obsoleto: "‚ö†Ô∏è Aten√ß√£o: essa informa√ß√£o pode estar desatualizada. Valide com o time respons√°vel antes de seguir."

{historico_conversa}CONTEXTO DISPON√çVEL:
{contexto}

PERGUNTA DO COLABORADOR:
{pergunta}

RESPOSTA (considere cccc e hist√≥rico ao responder):
    """
).strip()

KEYWORDS = ['ccc']


class AgenteccccAsync:
    """Agente especializado em cccc com opera√ß√µes ASS√çNCRONAS"""

    def __init__(self, *, debug: bool = False) -> None:
        self.config = SubagentConfig(
            identifier="cccc",
            name="ccccc",
            specialty="cccc",
            description="cccccc",
            keywords=KEYWORDS,
            table_name="knowledge_cccc",
            prompt_template=PROMPT_TEMPLATE,
            debug=debug
        )
        
        # Inicializar componentes ass√≠ncronos
        self.dal_async = PostgresDALAsync()
        self.llm = None
        self.embeddings = None
        self.memoria_conversas = {}
        
        # Configura√ß√£o de ferramentas MCP
        self.enable_mcp_tools = False
        self.mcp_tools_category = ""
        self.allowed_tools = []
        
        # Inicializar LLM e embeddings
        self._inicializar_llm()
    
    def _inicializar_llm(self):
        """Inicializa LLM e embeddings"""
        api_key = config.openai.api_key
        
        self.llm = ChatOpenAI(
            api_key=api_key,
            model="gpt-4o-mini",
            temperature=0.3,
            max_tokens=10000
        )
        
        self.embeddings = OpenAIEmbeddings(
            api_key=api_key,
            model=config.openai.embedding_model
        )
    
    async def processar_async(self, pergunta: str, user_profile: dict) -> str:
        """Processa pergunta de forma ASS√çNCRONA"""
        try:
            if self.config.debug:
                print(f"üîÑ [{self.config.name}] Processando pergunta (ASYNC): '{pergunta[:50]}...'")
            
            # Conectar ao banco de forma ass√≠ncrona
            await self.dal_async.connect()
            
            # Gerar embedding da pergunta
            query_embedding = await asyncio.to_thread(
                self.embeddings.embed_query,
                pergunta
            )
            
            # Buscar contexto relevante de forma ass√≠ncrona
            search_result = await self.dal_async.search_vectors_async(
                table_name=self.config.table_name,
                query_vector=query_embedding,
                limit=5,
                similarity_threshold=0.5
            )
            
            if self.config.debug:
                print(f"üìä [{self.config.name}] Encontrados {len(search_result.documents)} documentos relevantes")
            
            # Preparar contexto
            contexto_str = self._preparar_contexto(search_result.documents)
            
            # Preparar hist√≥rico
            usuario_id = f"{user_profile.get('Nome', 'usuario')}_{user_profile.get('Departamento', 'geral')}"
            historico_str = self._preparar_historico(usuario_id)
            
            # Preparar prompt
            prompt_final = self.config.prompt_template.format(
                historico_conversa=historico_str,
                contexto=contexto_str,
                pergunta=pergunta
            )
            
            # Gerar resposta de forma ass√≠ncrona
            if self.config.debug:
                print(f"ü§ñ [{self.config.name}] Gerando resposta com LLM (ASYNC)...")
            
            resposta = await asyncio.to_thread(
                self.llm.invoke,
                prompt_final
            )
            
            resposta_texto = resposta.content if hasattr(resposta, 'content') else str(resposta)
            
            # Armazenar na mem√≥ria
            self._adicionar_memoria(usuario_id, pergunta, resposta_texto)
            
            if self.config.debug:
                print(f"‚úÖ [{self.config.name}] Resposta gerada com {len(resposta_texto)} caracteres")
            
            return resposta_texto
            
        except Exception as e:
            error_msg = f"‚ùå Erro ao processar pergunta (Agente {self.config.name}): {str(e)}"
            print(error_msg)
            return "Desculpe, encontrei um erro ao processar sua pergunta sobre cccc. Por favor, tente novamente."
        
        finally:
            await self.dal_async.disconnect()
    
    def processar_pergunta(self, pergunta: str, user_profile: dict) -> str:
        """Wrapper s√≠ncrono para compatibilidade com sistema hier√°rquico"""
        return asyncio.run(self.processar_async(pergunta, user_profile))
    
    def _preparar_contexto(self, documents: list) -> str:
        """Prepara o contexto a partir dos documentos recuperados"""
        if not documents:
            return "Nenhum documento relevante encontrado na base de conhecimento."
        
        contexto_parts = []
        for i, doc in enumerate(documents, 1):
            conteudo = doc.get('conteudo', doc.get('content', ''))
            metadata_str = doc.get('metadata', {})
            
            contexto_parts.append(f"[Documento {i}]")
            if metadata_str:
                contexto_parts.append(f"Metadados: {metadata_str}")
            contexto_parts.append(conteudo)
            contexto_parts.append("")  # Linha em branco
        
        return "\n".join(contexto_parts)
    
    def _preparar_historico(self, usuario_id: str) -> str:
        """Prepara o hist√≥rico de conversas do usu√°rio"""
        if usuario_id not in self.memoria_conversas:
            return ""
        
        historico = self.memoria_conversas[usuario_id]
        if not historico:
            return ""
        
        # Pegar √∫ltimas 3 intera√ß√µes para contexto
        ultimas_interacoes = historico[-3:]
        historico_parts = ["HIST√ìRICO DA CONVERSA:"]
        
        for i, (pergunta_ant, resposta_ant) in enumerate(ultimas_interacoes, 1):
            historico_parts.append(f"[Intera√ß√£o {i}]")
            historico_parts.append(f"Pergunta: {pergunta_ant}")
            historico_parts.append(f"Resposta: {resposta_ant[:200]}...")
            historico_parts.append("")
        
        historico_parts.append("---")
        return "\n".join(historico_parts)
    
    def _adicionar_memoria(self, usuario_id: str, pergunta: str, resposta: str):
        """Adiciona intera√ß√£o √† mem√≥ria de conversas"""
        if usuario_id not in self.memoria_conversas:
            self.memoria_conversas[usuario_id] = []
        
        self.memoria_conversas[usuario_id].append((pergunta, resposta))
        
        # Manter apenas √∫ltimas 10 intera√ß√µes por usu√°rio
        if len(self.memoria_conversas[usuario_id]) > 10:
            self.memoria_conversas[usuario_id] = self.memoria_conversas[usuario_id][-10:]


def criar_agente_cccc_async(*, debug: bool = False) -> AgenteccccAsync:
    """Factory function para criar o agente de cccc ass√≠ncrono"""
    return AgenteccccAsync(debug=debug)


if __name__ == "__main__":
    import asyncio
    
    async def test_agente_cccc_async():
        agente = criar_agente_cccc_async(debug=True)
        
        perfil_demo = {
            "Nome": "Teste Usuario",
            "Cargo": "Analista",
            "Departamento": "cccc",
            "nivel_hierarquico": 2,
            "geografia": "BR",
            "projetos": ["N/A"],
        }

        perguntas_demo = [
            "Teste de pergunta para cccc",
        ]

        for pergunta in perguntas_demo:
            print(f"\n‚ùì Pergunta: {pergunta}")
            resposta = await agente.processar_async(pergunta, perfil_demo)
            print(f"ü§ñ ccccc: {resposta}")
            print("-" * 80)
    
    asyncio.run(test_agente_cccc_async())

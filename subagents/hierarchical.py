"""Interface e classes para suporte a agentes hierÃ¡rquicos com sub-especialistas."""

from __future__ import annotations

import logging
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, TYPE_CHECKING

import numpy as np
from langchain_openai import OpenAIEmbeddings

if TYPE_CHECKING:
    from subagents.base_subagent import BaseSubagent

logger = logging.getLogger(__name__)


@dataclass
class SubSpecialtyRule:
    """Regra para delegaÃ§Ã£o para sub-especialistas."""
    name: str
    target_subagent: str
    keywords: List[str]
    description: str
    confidence_threshold: float = 0.3
    priority: int = 1  # Maior nÃºmero = maior prioridade


class HierarchicalAgent(ABC):
    """Interface para agentes que podem ter sub-agentes especializados."""
    
    def __init__(self):
        self.sub_agents: Dict[str, 'BaseSubagent'] = {}
        self.subspecialty_rules: List[SubSpecialtyRule] = []
        self.delegation_history: List[str] = []
    
    def register_sub_agent(self, sub_agent: 'BaseSubagent') -> None:
        """Registra um sub-agente especializado."""
        identifier = sub_agent.config.identifier
        self.sub_agents[identifier] = sub_agent
        logger.info(f"Sub-agente registrado: {identifier} sob {self.__class__.__name__}")
    
    def add_subspecialty_rule(self, rule: SubSpecialtyRule) -> None:
        """Adiciona regra de sub-especializaÃ§Ã£o."""
        self.subspecialty_rules.append(rule)
        # Ordena por prioridade (maior nÃºmero primeiro)
        self.subspecialty_rules.sort(key=lambda r: r.priority, reverse=True)
    
    def find_best_subagent(self, query: str) -> Tuple[Optional[str], float]:
        """Encontra o melhor sub-agente para uma pergunta."""
        best_match = None
        best_score = 0.0
        
        query_lower = query.lower()
        print(f"ğŸ” Analisando palavras-chave em: '{query_lower}'")
        logger.debug(f"Analisando query: '{query_lower}'")
        
        for rule in self.subspecialty_rules:
            # Calcula score baseado nas keywords
            matches = 0
            matched_keywords = []
            
            for keyword in rule.keywords:
                if keyword.lower() in query_lower:
                    matches += 1
                    matched_keywords.append(keyword)
            
            if matches > 0:
                # Score baseado no nÃºmero de matches e peso das palavras
                score = matches / len(rule.keywords)
                
                print(f"ğŸ“ Regra '{rule.target_subagent}': {matches} matches {matched_keywords}, score: {score:.3f}")
                logger.debug(f"Regra '{rule.name}': {matches} matches {matched_keywords}, score: {score:.3f}, threshold: {rule.confidence_threshold}")
                
                if score >= rule.confidence_threshold and score > best_score:
                    best_match = rule.target_subagent
                    best_score = score
                    print(f"ğŸ¯ Nova melhor opÃ§Ã£o: {best_match} (score: {score:.3f})")
                    logger.debug(f"Nova melhor opÃ§Ã£o: {best_match} (score: {score:.3f})")
        
        logger.info(f"DelegaÃ§Ã£o hierÃ¡rquica: '{query[:50]}...' -> {best_match} (score: {best_score:.3f})")
        return best_match, best_score

    def find_top_candidates(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """
        Encontra os top-k candidatos para fallback chain.
        
        Args:
            query: Pergunta do usuÃ¡rio
            top_k: NÃºmero mÃ¡ximo de candidatos a retornar
            
        Returns:
            Lista ordenada de (agente, score) por score decrescente
        """
        candidates = []
        query_lower = query.lower()
        
        print(f"ğŸ” Analisando query: '{query_lower}'")
        print(f"ğŸ“‹ Total de regras: {len(self.subspecialty_rules)}")
        
        for rule in self.subspecialty_rules:
            matches = 0
            matched_keywords = []
            
            for keyword in rule.keywords:
                if keyword.lower() in query_lower:
                    matches += 1
                    matched_keywords.append(keyword)
            
            if matches > 0:
                score = matches / len(rule.keywords)
                print(f"ğŸ¯ Regra '{rule.name}' â†’ {matches} matches, score: {score:.3f}, threshold: {rule.confidence_threshold}")
                print(f"   Keywords matched: {matched_keywords}")
                
                if score >= rule.confidence_threshold:
                    candidates.append((rule.target_subagent, score))
                    print(f"   âœ… Adicionado candidato: {rule.target_subagent}")
                else:
                    print(f"   âŒ Score abaixo do threshold")
            else:
                print(f"âšª Regra '{rule.name}' â†’ 0 matches")
        
        # Ordenar por score decrescente e pegar os top-k
        candidates.sort(key=lambda x: x[1], reverse=True)
        print(f"ğŸ† Candidatos finais: {candidates}")
        return candidates[:top_k]
    
    @abstractmethod
    def process_with_hierarchy(self, query: str, user_profile: Dict) -> str:
        """Processa pergunta considerando hierarquia de sub-agentes."""
        pass


class TIHierarchicalAgent(HierarchicalAgent):
    """Agente de TI com capacidade hierÃ¡rquica."""
    
    def __init__(self, base_agent: 'BaseSubagent'):
        super().__init__()
        self.base_agent = base_agent
        self._setup_default_rules()
    
    def _setup_default_rules(self) -> None:
        """Configura regras padrÃ£o de sub-especializaÃ§Ã£o para TI."""
        
        # Governance - PolÃ­ticas, compliance, seguranÃ§a
        governance_rule = SubSpecialtyRule(
            name="governance_delegation",
            target_subagent="governance",
            keywords=[
                # PortuguÃªs
                "polÃ­tica", "compliance", "auditoria", "seguranÃ§a", "lgpd",
                "iso", "itil", "governanÃ§a", "riscos", "controle", "norma",
                "certificaÃ§Ã£o", "procedimento", "regulamentaÃ§Ã£o", "27001",
                "assinatura", "assinaturas", "eletrÃ´nica", "digital", "validaÃ§Ã£o",
                "senhas", "senha", "autenticaÃ§Ã£o", "conformidade",
                "abnt", "nbr", "13485", "9001", "14001", "45001",  # â† ADICIONADO
                "capÃ­tulo", "seÃ§Ã£o", "artigo", "clÃ¡usula", "requisito",  # â† ADICIONADO
                # InglÃªs
                "policy", "policies", "governance", "compliance", "audit", "security",
                "regulation", "standard", "certification", "procedure", "risk",
                "control", "signature", "signatures", "sign", "electronic", "digital",
                "validation", "password", "passwords", "authentication", "regulatory",
                "chapter", "section", "article", "clause", "requirement",  # â† ADICIONADO
                # Espanhol
                "polÃ­tica", "polÃ­ticas", "gobernanza", "cumplimiento", "auditorÃ­a",
                "firma", "firmas", "electrÃ³nica", "validaciÃ³n", "contraseÃ±a",
                "capÃ­tulo", "secciÃ³n", "artÃ­culo", "clÃ¡usula"  # â† ADICIONADO
            ],
            description="QuestÃµes de governanÃ§a, compliance e polÃ­ticas de TI",
            confidence_threshold=0.02,  # â† REDUZIDO de 0.03 para 0.02 (mais sensÃ­vel)
            priority=3
        )
        
        # Infraestrutura - Servidores, redes, hardware
        infra_rule = SubSpecialtyRule(
            name="infra_delegation",
            target_subagent="infra",
            keywords=[
                "servidor", "rede", "hardware", "datacenter", "backup",
                "disaster", "recovery", "uptime", "monitoramento", "capacidade",
                "performance", "storage", "virtualizaÃ§Ã£o", "cloud", "aws", "status"
            ],
            description="QuestÃµes de infraestrutura e operaÃ§Ãµes",
            confidence_threshold=0.05,  # Threshold mais baixo
            priority=2
        )
        
        # Desenvolvimento - AplicaÃ§Ãµes, sistemas, projetos
        dev_rule = SubSpecialtyRule(
            name="dev_delegation",
            target_subagent="dev",
            keywords=[
                # Palavras especÃ­ficas de desenvolvimento
                "desenvolvimento", "desenvolver", "aplicaÃ§Ã£o", "aplicativo", 
                "sistema", "cÃ³digo", "projeto", "software",
                "bug", "erro", "feature", "funcionalidade", "deploy", "release",
                "rest api", "api rest", "endpoint", "integraÃ§Ã£o api",  # â† API mais especÃ­fico
                "banco de dados", "database", "sql", "query",
                "integraÃ§Ã£o", "teste", "homologaÃ§Ã£o", "produÃ§Ã£o",
                "frontend", "backend", "fullstack", "microserviÃ§o",
                "git", "repositÃ³rio", "branch", "commit", "pull request",
                "docker", "container", "kubernetes", "devops"
            ],
            description="QuestÃµes de desenvolvimento e sistemas",
            confidence_threshold=0.05,
            priority=1
        )
        
        # Suporte ao UsuÃ¡rio Final - Senha, login, ferramentas bÃ¡sicas
        enduser_rule = SubSpecialtyRule(
            name="enduser_delegation",
            target_subagent="enduser",
            keywords=[
                "senha", "login", "acesso", "reset", "email", "outlook",
                "word", "excel", "teams", "usuario", "conta", "perfil",
                "suporte", "help", "ajuda", "como usar", "tutorial"
            ],
            description="QuestÃµes de suporte ao usuÃ¡rio final e ferramentas bÃ¡sicas",
            confidence_threshold=0.05,
            priority=4  # Maior prioridade para questÃµes de usuÃ¡rio
        )
        
        self.add_subspecialty_rule(enduser_rule)  # Adicionar primeiro (maior prioridade)
        self.add_subspecialty_rule(governance_rule)
        self.add_subspecialty_rule(infra_rule)
        self.add_subspecialty_rule(dev_rule)
    
    def _is_generic_response(self, response: str) -> bool:
        """Verifica se a resposta Ã© genÃ©rica (indica que o agente nÃ£o encontrou dados especÃ­ficos)."""
        generic_indicators = [
            "nÃ£o localizei informaÃ§Ãµes especÃ­ficas sobre",
            "nÃ£o localizei essa informaÃ§Ã£o especÃ­fica",
            "nÃ£o tenho informaÃ§Ãµes especÃ­ficas",
            "nÃ£o encontrei dados especÃ­ficos",
            "preciso de mais informaÃ§Ãµes",
            "nÃ£o possuo dados detalhados",
            "nÃ£o encontrei essa informaÃ§Ã£o",
            "nÃ£o tenho acesso a essa informaÃ§Ã£o",
            "informaÃ§Ãµes nÃ£o disponÃ­veis",
            "dados nÃ£o encontrados",
            "nÃ£o consta em nossa base",
            "nÃ£o foi possÃ­vel localizar",
            "nÃ£o possuo informaÃ§Ãµes sobre",
            "nÃ£o hÃ¡ dados disponÃ­veis",
            "informaÃ§Ã£o nÃ£o encontrada",
            "desculpe, mas nÃ£o encontrei",
            "lamento, mas nÃ£o possuo"
        ]
        
        response_lower = response.lower()
        is_generic = any(indicator in response_lower for indicator in generic_indicators)
        
        # Debug logging para acompanhar detecÃ§Ã£o
        if is_generic:
            print(f"ğŸ” Resposta genÃ©rica detectada: '{response[:100]}...'")
        
        return is_generic
    
    def _validate_response_quality(
        self, 
        query: str, 
        response: str, 
        context_docs: Optional[List[Dict]] = None
    ) -> Tuple[bool, float, Dict[str, float]]:
        """
        ValidaÃ§Ã£o multi-critÃ©rio da qualidade da resposta.
        
        Args:
            query: Pergunta original do usuÃ¡rio
            response: Resposta gerada pelo agente
            context_docs: Documentos usados como contexto (opcional)
            
        Returns:
            (is_valid, overall_score, detailed_scores)
            - is_valid: True se a resposta passa no threshold geral (>= 0.70)
            - overall_score: Score ponderado final (0-1)
            - detailed_scores: DicionÃ¡rio com scores individuais de cada critÃ©rio
        """
        scores = {}
        
        # 1. CRITÃ‰RIO: Especificidade (35% do peso)
        # Verifica se resposta nÃ£o Ã© genÃ©rica e tem conteÃºdo substantivo
        specificity_score = self._check_specificity(response)
        scores['specificity'] = specificity_score
        
        # 2. CRITÃ‰RIO: RelevÃ¢ncia SemÃ¢ntica (35% do peso)
        # Verifica se resposta estÃ¡ semanticamente relacionada Ã  pergunta
        relevance_score = self._check_semantic_relevance(query, response)
        scores['relevance'] = relevance_score
        
        # 3. CRITÃ‰RIO: CitaÃ§Ã£o de Fontes (20% do peso)
        # Verifica se resposta menciona documentos e nÃ£o inventa informaÃ§Ãµes
        citation_score = self._check_citations(response, context_docs)
        scores['citations'] = citation_score
        
        # 4. CRITÃ‰RIO: Completude (10% do peso)
        # Verifica se resposta tem profundidade adequada
        completeness_score = self._check_completeness(response)
        scores['completeness'] = completeness_score
        
        # Calcular score ponderado final
        overall_score = (
            specificity_score * 0.35 +
            relevance_score * 0.35 +
            citation_score * 0.20 +
            completeness_score * 0.10
        )

        # Threshold para aprovaÃ§Ã£o: 0.50 (50%)
        is_valid = overall_score >= 0.20
        
        # Log detalhado para debugging
        print(f"\n{'='*60}")
        print(f"ğŸ” VALIDAÃ‡ÃƒO DE QUALIDADE DA RESPOSTA")
        print(f"{'='*60}")
        print(f"ğŸ“Š Especificidade:      {specificity_score:.2f} (35% peso)")
        print(f"ğŸ¯ RelevÃ¢ncia SemÃ¢ntica: {relevance_score:.2f} (35% peso)")
        print(f"ğŸ“š CitaÃ§Ã£o de Fontes:   {citation_score:.2f} (20% peso)")
        print(f"âœ… Completude:          {completeness_score:.2f} (10% peso)")
        print(f"{'â”€'*60}")
        print(f"ğŸ† SCORE FINAL: {overall_score:.2f} ({'âœ… APROVADO' if is_valid else 'âŒ REJEITADO'})")
        print(f"{'='*60}\n")
        
        return is_valid, overall_score, scores
    
    def _check_specificity(self, response: str) -> float:
        """
        Verifica se a resposta Ã© especÃ­fica e nÃ£o genÃ©rica.
        
        Retorna score de 0 a 1:
        - 0.0: Completamente genÃ©rica
        - 1.0: Altamente especÃ­fica
        """
        # 1. Verificar frases genÃ©ricas (mÃ©todo existente)
        if self._is_generic_response(response):
            return 0.0
        
        # 2. Verificar comprimento mÃ­nimo
        if len(response.split()) < 30:
            return 0.3  # Muito curta, provavelmente superficial
        
        # 3. Verificar presenÃ§a de informaÃ§Ãµes tÃ©cnicas/especÃ­ficas
        # Indicadores de especificidade: nÃºmeros, datas, nomes prÃ³prios, termos tÃ©cnicos
        specificity_indicators = 0
        
        # NÃºmeros e percentuais
        if re.search(r'\d+', response):
            specificity_indicators += 1
        
        # Datas
        if re.search(r'\b(20\d{2}|19\d{2})\b', response):
            specificity_indicators += 1
        
        # Nomes prÃ³prios (palavras capitalizadas no meio do texto)
        proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', response)
        if len(proper_nouns) >= 2:
            specificity_indicators += 1
        
        # Termos tÃ©cnicos comuns em documentaÃ§Ã£o
        technical_terms = [
            'sistema', 'processo', 'procedimento', 'requisito', 'norma',
            'polÃ­tica', 'regulamentaÃ§Ã£o', 'compliance', 'auditoria',
            'configuraÃ§Ã£o', 'implementaÃ§Ã£o', 'validaÃ§Ã£o', 'verificaÃ§Ã£o'
        ]
        response_lower = response.lower()
        if sum(1 for term in technical_terms if term in response_lower) >= 2:
            specificity_indicators += 1
        
        # Score baseado em indicadores (mÃ¡ximo 4)
        specificity_score = min(1.0, 0.5 + (specificity_indicators * 0.15))
        
        return specificity_score
    
    def _check_semantic_relevance(self, query: str, response: str) -> float:
        """
        Verifica relevÃ¢ncia semÃ¢ntica entre pergunta e resposta usando embeddings.
        
        Retorna score de 0 a 1 baseado em similaridade coseno.
        """
        try:
            # Inicializar embeddings (lazy loading)
            if not hasattr(self, '_embeddings'):
                self._embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            
            # Gerar embeddings
            query_embedding = self._embeddings.embed_query(query)
            response_embedding = self._embeddings.embed_query(response[:1000])  # Limitar tamanho
            
            # Calcular similaridade coseno
            query_vec = np.array(query_embedding)
            response_vec = np.array(response_embedding)
            
            cosine_sim = np.dot(query_vec, response_vec) / (
                np.linalg.norm(query_vec) * np.linalg.norm(response_vec)
            )
            
            # Normalizar para 0-1 (cosine jÃ¡ retorna -1 a 1, mas em prÃ¡tica Ã© 0-1 para textos)
            relevance_score = max(0.0, min(1.0, cosine_sim))
            
            return relevance_score
            
        except Exception as e:
            logger.warning(f"Erro ao calcular relevÃ¢ncia semÃ¢ntica: {e}")
            # Fallback: se houver erro, assumir relevÃ¢ncia moderada
            return 0.65
    
    def _check_citations(self, response: str, context_docs: Optional[List[Dict]]) -> float:
        """
        Verifica se resposta cita fontes do contexto e nÃ£o inventa informaÃ§Ãµes.
        
        Retorna score de 0 a 1:
        - 1.0: Cita mÃºltiplas fontes claramente
        - 0.5-0.8: Cita algumas fontes
        - 0.0-0.4: NÃ£o cita fontes ou inventa informaÃ§Ãµes
        """
        # Se nÃ£o hÃ¡ contexto disponÃ­vel, nÃ£o podemos validar citaÃ§Ãµes
        if not context_docs:
            # Verificar se resposta pelo menos menciona buscar/consultar documentos
            if any(term in response.lower() for term in ['documento', 'polÃ­tica', 'norma', 'procedimento', 'regulamentaÃ§Ã£o']):
                return 0.6  # Menciona conceitos gerais de documentaÃ§Ã£o
            return 0.4  # Sem contexto e sem menÃ§Ã£o a fontes
        
        response_lower = response.lower()
        citations_found = 0
        
        # Procurar menÃ§Ãµes aos documentos do contexto
        for doc in context_docs:
            doc_title = doc.get('titulo', '').lower()
            doc_source = doc.get('fonte', '').lower()
            
            # Verificar se tÃ­tulo ou fonte sÃ£o mencionados na resposta
            if doc_title and len(doc_title) > 10:  # Ignorar tÃ­tulos muito curtos
                # Procurar por match parcial (pelo menos 60% das palavras principais)
                title_words = [w for w in doc_title.split() if len(w) > 3]
                if title_words:
                    matches = sum(1 for word in title_words if word in response_lower)
                    if matches / len(title_words) >= 0.6:
                        citations_found += 1
                        continue
            
            # Verificar fonte (ISO, RDC, FDA, etc.)
            if doc_source and doc_source in response_lower:
                citations_found += 1
        
        # Calcular score baseado em nÃºmero de citaÃ§Ãµes
        num_docs = len(context_docs)
        if num_docs == 0:
            return 0.5
        
        citation_ratio = citations_found / num_docs
        
        # Score progressivo:
        # - 0 citaÃ§Ãµes: 0.0
        # - 1-25% citaÃ§Ãµes: 0.5
        # - 26-50% citaÃ§Ãµes: 0.7
        # - 51-75% citaÃ§Ãµes: 0.85
        # - 76-100% citaÃ§Ãµes: 1.0
        if citation_ratio == 0:
            return 0.0
        elif citation_ratio <= 0.25:
            return 0.5
        elif citation_ratio <= 0.50:
            return 0.7
        elif citation_ratio <= 0.75:
            return 0.85
        else:
            return 1.0
    
    def _check_completeness(self, response: str) -> float:
        """
        Verifica se resposta tem profundidade e completude adequadas.
        
        Retorna score de 0 a 1 baseado em:
        - Comprimento adequado
        - Estrutura (mÃºltiplos parÃ¡grafos)
        - PresenÃ§a de listas ou enumeraÃ§Ãµes
        - ConclusÃ£o ou prÃ³ximos passos
        """
        words = response.split()
        num_words = len(words)
        
        score = 0.0
        
        # 1. Comprimento adequado (40% do critÃ©rio)
        if num_words < 50:
            score += 0.0
        elif num_words < 100:
            score += 0.2
        elif num_words < 200:
            score += 0.3
        else:
            score += 0.4
        
        # 2. Estrutura em parÃ¡grafos (30% do critÃ©rio)
        paragraphs = [p.strip() for p in response.split('\n\n') if p.strip()]
        if len(paragraphs) >= 2:
            score += 0.3
        elif len(paragraphs) == 1 and num_words > 100:
            score += 0.15
        
        # 3. Listas ou enumeraÃ§Ãµes (20% do critÃ©rio)
        # Detectar bullets, nÃºmeros, ou estruturas enumeradas
        has_lists = any(char in response for char in ['â€¢', 'â—', 'â—¦', 'â–ª']) or \
                   bool(re.search(r'\n\s*[-*]\s+', response)) or \
                   bool(re.search(r'\n\s*\d+[.)]\s+', response))
        
        if has_lists:
            score += 0.2
        
        # 4. ConclusÃ£o ou prÃ³ximos passos (10% do critÃ©rio)
        conclusion_indicators = [
            'portanto', 'assim', 'dessa forma', 'em resumo', 'concluindo',
            'prÃ³ximos passos', 'recomendo', 'sugiro', 'vocÃª pode',
            'para mais informaÃ§Ãµes', 'consulte', 'entre em contato'
        ]
        
        response_lower = response.lower()
        has_conclusion = any(indicator in response_lower for indicator in conclusion_indicators)
        
        if has_conclusion:
            score += 0.1
        
        return min(1.0, score)
    
    def _get_delegation_reason(self, query: str, agent_type: str, score: float) -> str:
        """Explica o motivo da delegaÃ§Ã£o para transparÃªncia."""
        query_lower = query.lower()
        
        reasons = {
            'governance': {
                'keywords': ['regulamentaÃ§Ã£o', 'anvisa', 'polÃ­tica', 'compliance', 'seguranÃ§a', 'auditoria', 'norma', 'lei', 'decreto'],
                'description': 'Especialista em regulamentaÃ§Ãµes, polÃ­ticas e compliance'
            },
            'infrastructure': {
                'keywords': ['servidor', 'rede', 'infraestrutura', 'hardware', 'sistema', 'performance'],  
                'description': 'Especialista em infraestrutura e sistemas'
            },
            'dev': {
                'keywords': ['desenvolvimento', 'cÃ³digo', 'programaÃ§Ã£o', 'api', 'software', 'aplicaÃ§Ã£o'],
                'description': 'Especialista em desenvolvimento de software'
            },
            'enduser': {
                'keywords': ['usuÃ¡rio', 'interface', 'suporte', 'treinamento', 'manual', 'tutorial'],
                'description': 'Especialista em suporte ao usuÃ¡rio final'
            }
        }
        
        if agent_type in reasons:
            agent_info = reasons[agent_type]
            matched_keywords = [kw for kw in agent_info['keywords'] if kw in query_lower]
            
            if matched_keywords:
                return f"{agent_info['description']}. Palavras-chave identificadas: {', '.join(matched_keywords)}"
            else:
                return f"{agent_info['description']}. Score de relevÃ¢ncia: {score:.3f}"
        
        return f"Agente com maior score de relevÃ¢ncia ({score:.3f}) para esta consulta"
    
    def process_with_hierarchy(self, query: str, user_profile: Dict) -> str:
        """Processa pergunta usando hierarquia de sub-especialistas com fallback chain."""
        
        # Inicializar cadeia de decisÃ£o para transparÃªncia
        decision_chain = []
        decision_chain.append(f"ğŸ” **AnÃ¡lise inicial**: TI Hierarchy analisando pergunta sobre '{query[:50]}...'")
        
        # 1. Obter top 3 candidatos para fallback chain
        candidates = self.find_top_candidates(query, top_k=3)
        
        if candidates:
            candidate_info = [(agent, f'{score:.3f}') for agent, score in candidates]
            decision_chain.append(f"ğŸ¯ **Candidatos identificados**: {candidate_info}")
            decision_chain.append("ğŸ“Š **CritÃ©rio de seleÃ§Ã£o**: RelevÃ¢ncia por palavras-chave e especialidade")
            
            # 2. Tentar cada candidato em ordem de score
            for i, (candidate_agent, score) in enumerate(candidates):
                if candidate_agent not in self.sub_agents:
                    continue
                    
                sub_agent = self.sub_agents[candidate_agent]
                
                # Log da delegaÃ§Ã£o com motivo
                delegation_reason = self._get_delegation_reason(query, candidate_agent, score)
                decision_chain.append(f"ğŸ”„ **Tentativa #{i+1}**: Delegando para **{sub_agent.config.name}** (score: {score:.3f})")
                decision_chain.append(f"ğŸ’¡ **Motivo**: {delegation_reason}")
                
                logger.info(f"ğŸ”„ TI delegando para sub-especialista: {sub_agent.config.name}")
                
                try:
                    # Processa com o sub-agente especializado
                    print(f"ğŸ¤– Chamando {sub_agent.config.name} (tabela: {sub_agent.config.table_name})")
                    result = sub_agent.processar_pergunta(query, user_profile)
                    
                    print(f"ğŸ“ {sub_agent.config.name} retornou {len(result)} caracteres")
                    print(f"ğŸ” Primeiros 100 chars: '{result[:100]}...'")
                    
                    # VALIDAÃ‡ÃƒO ROBUSTA DE QUALIDADE (4 critÃ©rios)
                    # Tentar obter documentos do contexto (se o sub-agente expÃ´s isso)
                    context_docs = None
                    if hasattr(sub_agent, '_last_context_docs'):
                        context_docs = sub_agent._last_context_docs
                    
                    # Se a resposta Ã© uma mensagem de erro informativa, aceitar sem validar
                    is_error_message = (
                        result.startswith("âš ï¸") or 
                        result.startswith("Desculpe") or
                        "Problema de Conectividade" in result or
                        "Timeout" in result
                    )
                    
                    if is_error_message:
                        # Aceitar mensagem de erro informativa
                        print(f"âš ï¸ {sub_agent.config.name} retornou mensagem de erro informativa")
                        decision_chain.append(f"âš ï¸ **Resultado**: {sub_agent.config.name} encontrou um problema tÃ©cnico")
                        decision_chain.append("ğŸ’¡ **AÃ§Ã£o**: Retornando mensagem informativa ao usuÃ¡rio")
                        
                        # Montar transparÃªncia
                        transparency_section = "\n\n" + "="*60 + "\n"
                        transparency_section += "ğŸ§  **CADEIA DE DECISÃƒO E RACIOCÃNIO**\n"
                        transparency_section += "="*60 + "\n"
                        for step in decision_chain:
                            transparency_section += f"{step}\n"
                        
                        transparency_section += f"\nğŸ“‹ **Resposta fornecida por**: {sub_agent.config.name} ({sub_agent.config.specialty})"
                        transparency_section += "\nâš ï¸ **Status**: Problema tÃ©cnico detectado"
                        transparency_section += "\nğŸ¯ **Coordenado por**: Sistema TI HierÃ¡rquico"
                        transparency_section += "\n" + "="*60
                        
                        return result + transparency_section
                    
                    # Validar qualidade da resposta
                    is_valid, quality_score, detailed_scores = self._validate_response_quality(
                        query=query,
                        response=result,
                        context_docs=context_docs
                    )
                    
                    # Log dos resultados da validaÃ§Ã£o
                    if not is_valid:
                        print(f"âŒ Resposta rejeitada por {sub_agent.config.name} (score: {quality_score:.2f})")
                        decision_chain.append(f"âŒ **Resultado**: Resposta de {sub_agent.config.name} nÃ£o passou na validaÃ§Ã£o de qualidade")
                        decision_chain.append(f"ğŸ“Š **Score de Qualidade**: {quality_score:.2f}/1.00 (threshold: 0.70)")
                        decision_chain.append(f"ğŸ“ˆ **Detalhamento**: Especificidade {detailed_scores['specificity']:.2f}, RelevÃ¢ncia {detailed_scores['relevance']:.2f}, CitaÃ§Ãµes {detailed_scores['citations']:.2f}, Completude {detailed_scores['completeness']:.2f}")
                        
                        if i < len(candidates) - 1:
                            decision_chain.append("âš¡ **AÃ§Ã£o**: Tentando prÃ³ximo especialista na hierarquia...")
                        continue
                    
                    # Sucesso! Adiciona cadeia de decisÃ£o transparente
                    print(f"âœ… Resposta aprovada (score: {quality_score:.2f})")
                    decision_chain.append(f"âœ… **Sucesso**: {sub_agent.config.name} forneceu resposta de qualidade!")
                    decision_chain.append(f"ğŸ“Š **Score de Qualidade**: {quality_score:.2f}/1.00")
                    decision_chain.append(f"ğŸ“ˆ **Detalhamento**: Especificidade {detailed_scores['specificity']:.2f}, RelevÃ¢ncia {detailed_scores['relevance']:.2f}, CitaÃ§Ãµes {detailed_scores['citations']:.2f}, Completude {detailed_scores['completeness']:.2f}")
                    
                    # Montar resposta com transparÃªncia completa
                    transparency_section = "\n\n" + "="*60 + "\n"
                    transparency_section += "ğŸ§  **CADEIA DE DECISÃƒO E RACIOCÃNIO**\n"
                    transparency_section += "="*60 + "\n"
                    for step in decision_chain:
                        transparency_section += f"{step}\n"
                    
                    transparency_section += f"\nğŸ“‹ **Resposta final fornecida por**: {sub_agent.config.name} ({sub_agent.config.specialty})"
                    if i > 0:
                        transparency_section += f"\nğŸ”„ **Redirecionamentos**: {i} tentativa(s) anteriores"
                    transparency_section += "\nğŸ¯ **Coordenado por**: Sistema TI HierÃ¡rquico"
                    transparency_section += "\n" + "="*60
                    
                    self.delegation_history.append(f"{query[:50]}... -> {candidate_agent}")
                    
                    return result + transparency_section
                    
                except Exception as e:
                    logger.error(f"Erro no sub-agente {candidate_agent}: {e}")
                    print(f"âŒ Erro em {sub_agent.config.name}, tentando prÃ³ximo...")
                    continue
        else:
            decision_chain.append("â“ **Resultado da anÃ¡lise**: Nenhum especialista especÃ­fico identificado")
            decision_chain.append("ğŸ”„ **AÃ§Ã£o**: Redirecionando diretamente para TI geral")
        
        # 3. Se nenhum candidato funcionou, usar TI principal (se disponÃ­vel)
        decision_chain.append("âŒ **Resultado**: Nenhum especialista encontrou informaÃ§Ãµes especÃ­ficas")
        
        if self.base_agent:
            decision_chain.append("ğŸ”„ **Fallback final**: Redirecionando para agente TI geral")
            logger.info("ğŸ¤– TI processando com conhecimento geral")
            result = self.base_agent.processar_pergunta(query, user_profile)
            
            # Montar transparÃªncia para fallback final
            transparency_section = "\n\n" + "="*60 + "\n"
            transparency_section += "ğŸ§  **CADEIA DE DECISÃƒO E RACIOCÃNIO**\n"
            transparency_section += "="*60 + "\n"
            for step in decision_chain:
                transparency_section += f"{step}\n"
            
            transparency_section += f"\nğŸ“‹ **Resposta final fornecida por**: {self.base_agent.config.name} (TI Geral)"
            transparency_section += "\nâš ï¸ **Motivo**: Especialistas nÃ£o encontraram informaÃ§Ãµes especÃ­ficas"
            transparency_section += "\nğŸ¯ **Coordenado por**: Sistema TI HierÃ¡rquico"
            transparency_section += "\n" + "="*60
            
            return result + transparency_section
        else:
            # Sem base_agent disponÃ­vel, retornar mensagem informativa
            decision_chain.append("âš ï¸ **Fallback**: Agente TI geral nÃ£o disponÃ­vel")
            
            logger.warning("âš ï¸ Base agent TI nÃ£o disponÃ­vel e nenhum especialista respondeu")
            
            # Montar resposta com transparÃªncia
            transparency_section = "\n\n" + "="*60 + "\n"
            transparency_section += "ğŸ§  **CADEIA DE DECISÃƒO E RACIOCÃNIO**\n"
            transparency_section += "="*60 + "\n"
            for step in decision_chain:
                transparency_section += f"{step}\n"
            
            transparency_section += "\nâš ï¸ **Status**: Sistema em configuraÃ§Ã£o"
            transparency_section += "\nğŸ’¡ **SugestÃ£o**: Tente reformular a pergunta de forma mais especÃ­fica"
            transparency_section += "\nğŸ¯ **Coordenado por**: Sistema TI HierÃ¡rquico"
            transparency_section += "\n" + "="*60
            
            result = (
                "Desculpe, nÃ£o consegui processar sua pergunta sobre TI no momento.\n\n"
                "Por favor, tente reformular sua pergunta de forma mais especÃ­fica, mencionando:\n"
                "- **GovernanÃ§a**: PolÃ­ticas, seguranÃ§a, compliance\n"
                "- **Desenvolvimento**: AplicaÃ§Ãµes, sistemas, integraÃ§Ãµes\n"
                "- **Infraestrutura**: Servidores, redes, hardware\n"
                "- **Suporte**: Problemas de usuÃ¡rios, tickets, acesso"
            )
            
            return result + transparency_section
    
    def get_hierarchy_stats(self) -> Dict:
        """Retorna estatÃ­sticas da hierarquia."""
        return {
            "sub_agents_count": len(self.sub_agents),
            "sub_agents": list(self.sub_agents.keys()),
            "rules_count": len(self.subspecialty_rules),
            "delegations_history": len(self.delegation_history),
            "recent_delegations": self.delegation_history[-5:] if self.delegation_history else []
        }
